{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12479618,"sourceType":"datasetVersion","datasetId":7715214}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport xgboost as xgb\nimport numpy as np\nfrom scipy.stats import uniform, randint\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_error,\n    r2_score,\n     accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score\n)\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\n\n#progress bar\nfrom tqdm import trange\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-15T21:25:36.012710Z","iopub.execute_input":"2025-07-15T21:25:36.013435Z","iopub.status.idle":"2025-07-15T21:25:36.018799Z","shell.execute_reply.started":"2025-07-15T21:25:36.013410Z","shell.execute_reply":"2025-07-15T21:25:36.018110Z"}},"outputs":[],"execution_count":201},{"cell_type":"code","source":"torch.set_default_dtype(torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T21:25:36.020951Z","iopub.execute_input":"2025-07-15T21:25:36.021198Z","iopub.status.idle":"2025-07-15T21:25:36.034001Z","shell.execute_reply.started":"2025-07-15T21:25:36.021176Z","shell.execute_reply":"2025-07-15T21:25:36.033305Z"}},"outputs":[],"execution_count":202},{"cell_type":"code","source":"use_cuda = True\nuse_cuda = False if not use_cuda else torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\ntorch.cuda.get_device_name(device) if use_cuda else 'cpu'\nprint('Using device', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T21:25:36.035042Z","iopub.execute_input":"2025-07-15T21:25:36.035306Z","iopub.status.idle":"2025-07-15T21:25:36.049184Z","shell.execute_reply.started":"2025-07-15T21:25:36.035285Z","shell.execute_reply":"2025-07-15T21:25:36.048558Z"}},"outputs":[{"name":"stdout","text":"Using device cuda:0\n","output_type":"stream"}],"execution_count":203},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\ndata = pd.read_csv(\"/kaggle/input/airbnbdata-barcelona/listing_data_lower_dimension.csv\")\ndata = data.select_dtypes(exclude=['object', 'string']).dropna(subset=['price', 'id'])\n\nX = data.drop(columns=['price','id'])\ny = data['price']\n\n# --- 1) Split as before ---\nX_train_df, X_test_df, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# --- 2) Impute, but don’t overwrite the DataFrames! ---\nimputer = SimpleImputer(strategy='median')\n\n# Fit on the DataFrame’s values and get back NumPy arrays\nX_train_imp = imputer.fit_transform(X_train_df)\nX_test_imp  = imputer.transform(X_test_df)\n\n# --- 3) Re‑wrap into DataFrames using the *original* indices ---\ncols = X.columns\nX_train = pd.DataFrame(X_train_imp, columns=cols, index=X_train_df.index)\nX_test  = pd.DataFrame(X_test_imp,  columns=cols, index=X_test_df.index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T21:25:36.050222Z","iopub.execute_input":"2025-07-15T21:25:36.050427Z","iopub.status.idle":"2025-07-15T21:25:39.407574Z","shell.execute_reply.started":"2025-07-15T21:25:36.050412Z","shell.execute_reply":"2025-07-15T21:25:39.406796Z"}},"outputs":[],"execution_count":204},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(\n    objective='reg:squarederror',\n    tree_method='gpu_hist',\n    verbosity=0,       # 0 = silent, 1 = warning, 2 = info, 3 = debug\n    random_state=42,\n    early_stopping_rounds = 10,\n)\n\nparam_dist = {\n    'max_depth':        randint(12, 17),\n    'learning_rate':    uniform(0.005, 0.015),\n    'n_estimators':     randint(1000, 2000),\n    'subsample':        uniform(0.6, 0.4),\n    'colsample_bytree': uniform(0.6, 0.4),\n    'gamma': uniform(0, 10),        # minimum loss reduction to make a split\n    'reg_alpha': uniform(0, 5),    # L1 regularization\n    'reg_lambda': uniform(0, 5),   # L2 regularization\n}\n\nrandom_search = RandomizedSearchCV(\n    estimator=my_model,\n    param_distributions=param_dist,\n    n_iter= 50,                         # wie viele Kombinationen wir testen\n    scoring='neg_root_mean_squared_error',\n    cv=3,\n    verbose=1,\n    random_state=42,\n    n_jobs=-1,\n    refit = True\n)\nrandom_search.fit(\n            X_train, y_train, \n            eval_set=[(X_test, y_test)], \n            verbose=False)\n\nprint(\"\\n=== RandomizedSearchCV ===\")\nprint(\"Best Params:\", random_search.best_params_)\nprint(\"Best RMSE (CV):\", -random_search.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T21:25:39.408933Z","iopub.execute_input":"2025-07-15T21:25:39.409203Z"}},"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 50 candidates, totalling 150 fits\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# 1) Convert the cv_results_ dict into a DataFrame\ncv_results = pd.DataFrame(random_search.cv_results_)\n\n# 2) Sort by the rank (1 = best) so the top rows are your best trials\ncv_results = cv_results.sort_values('rank_test_score')\n\n# 3) Select the columns you want to see\ncols_of_interest = [\n    'rank_test_score',\n    'mean_test_score',\n    'std_test_score',\n    'param_max_depth',\n    'param_learning_rate',\n    'param_n_estimators',\n    'param_subsample',\n    'param_colsample_bytree'\n]\n\n# 4) Display the top 10\nprint(cv_results[cols_of_interest].head(10))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model = random_search.best_estimator_  \ny_pred_tuned = best_model.predict(X_test_imp)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, X_test, y_test):\n    \"\"\"\n    Führt Vorhersage auf X_test durch und berechnet verschiedene Kennzahlen.\n    Gibt y_pred zurück, damit du es z.B. für Plots weiterverwenden kannst.\n    \"\"\"\n    # 1) Vorhersagen\n    y_pred = model.predict(X_test)\n\n    # 2) Metriken berechnen\n    mse  = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    mae  = mean_absolute_error(y_test, y_pred)\n    r2   = r2_score(y_test, y_pred)\n\n    # 3) Ausgabe\n    print(f\"MSE:  {mse:.4f}\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"MAE:  {mae:.4f}\")\n    print(f\"R²:   {r2:.4f}\")\n\n    return y_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import plot_importance\nfrom sklearn.model_selection import learning_curve\ndef plot_accuracy(y_true, y_pred):\n    \"\"\"\n    Zeichnet:\n      1) True-vs-Predicted-Scatter mit 45°-Linie\n      2) Residual-Distribution (Histogramm)\n\n    \"\"\"\n\n\n    cv_df = pd.DataFrame(random_search.cv_results_)\n    # 2) Convert negative RMSE back to positive\n    cv_df['cv_rmse'] = -cv_df['mean_test_score']\n    \n    # 3) Scatter‑plot max_depth against CV RMSE\n    plt.figure(figsize=(8, 5))\n    plt.scatter(cv_df['param_max_depth'], cv_df['cv_rmse'])\n    plt.xlabel('max_depth')\n    plt.ylabel('CV RMSE')\n    plt.title('max_depth vs. CV RMSE')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n\n    cv_df = pd.DataFrame(random_search.cv_results_)\n    plt.scatter(cv_df['param_learning_rate'], -cv_df['mean_test_score'])\n    plt.xlabel('learning_rate'); plt.ylabel('CV RMSE'); plt.show()\n    # 1) Absolute Errors berechnen\n    abs_errors = np.abs(y_test - y_pred)\n#    plt.figure(figsize=(8,5))\n#    plt.hist(abs_errors, bins=30, edgecolor='k', alpha=0.7,\n#             weights=np.ones_like(abs_errors)/len(abs_errors)*100)\n#    plt.xlim(0, 500)  # Fokus auf Fehler bis 500\n#    plt.xlabel('Absolute Error')\n#    plt.ylabel('Percentage (%)')\n#    plt.title('Absolute Errors (0–500)')\n#    plt.grid(True, linestyle='--', alpha=0.7)\n#    plt.tight_layout()\n\n\n    threshold = np.percentile(abs_errors, 95)\n    errs_clip = abs_errors[abs_errors <= threshold]\n    plt.figure(figsize=(8,5))\n    plt.hist(errs_clip, bins=30, edgecolor='k', alpha=0.7,\n             weights=np.ones_like(errs_clip)/len(abs_errors)*100)\n    plt.xlabel(f'Absolute Error (bis {threshold:.0f})')\n    plt.ylabel('Percentage (%)')\n    plt.title(f'Errors bis zum 95. Perzentil ({threshold:.0f})')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n\n    \n    # Scatter True vs Predicted\n    plt.figure(figsize=(6,6))\n    lims = [min(min(y_true), min(y_pred)), max(max(y_true), max(y_pred))]\n    plt.scatter(y_true, y_pred, alpha=0.5)\n    plt.plot(lims, lims, 'k--', linewidth=1)\n    plt.xlabel(\"True Values\")\n    plt.xlim(0, 500)\n    plt.ylim(0, 500)\n    plt.ylabel(\"Predicted Values\")\n    plt.title(\"True vs. Predicted\")\n    plt.tight_layout()\n    plt.show()\n\n#\n#    plt.figure(figsize=(8,5))\n#    plt.hist(y_test,  bins=30, alpha=0.5, label='True Values')\n#    plt.hist(y_pred,  bins=30, alpha=0.5, label='Predicted Values')\n#    plt.xlabel('Preis')\n #   plt.ylabel('Häufigkeit')\n#    plt.title('Histogramm: True vs. Predicted')\n#    plt.legend()\n#    plt.tight_layout()\n\n    # Histogramm der Residuen\n#    residuals = y_true - y_pred\n#    plt.figure(figsize=(6,4))\n#    plt.hist(residuals, bins=30, edgecolor='k', alpha=0.7)\n#    plt.xlabel(\"Residual (True – Predicted)\")\n#    plt.ylabel(\"Frequency\")\n#    plt.title(\"Residual Distribution\")\n#    plt.tight_layout()\n\n\n\n\n\n    \n    plot_importance(best_model, max_num_features=10, importance_type='gain')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#evaluate\nevaluate_model(best_model, X_test, y_test)\nplot_accuracy(y_test, y_pred_tuned)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}