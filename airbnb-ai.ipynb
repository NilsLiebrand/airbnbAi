{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12090492,"sourceType":"datasetVersion","datasetId":7611132}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Airbnb-AI\nA ml-project to predict rent of an airbnb flat in Barcelona\nNils Liebrand, Till Malte Friedrich, Than Tran","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport requests\nimport unicodedata\nfrom PIL import Image\nfrom io import BytesIO\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport zipfile\nimport ast\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:48:21.350705Z","iopub.execute_input":"2025-06-24T07:48:21.351489Z","iopub.status.idle":"2025-06-24T07:48:21.371746Z","shell.execute_reply.started":"2025-06-24T07:48:21.351458Z","shell.execute_reply":"2025-06-24T07:48:21.370635Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extracing the data","metadata":{}},{"cell_type":"code","source":"listing_data = pd.read_csv('/kaggle/input/barcelona-airbnb/barcelona_airbnbdata/listings.csv', encoding=\"latin1\")\nlisting_details_data = pd.read_csv('/kaggle/input/barcelona-airbnb/barcelona_airbnbdata/listings_2.csv' , encoding=\"latin1\")\nreview_data = pd.read_csv('/kaggle/input/barcelona-airbnb/barcelona_airbnbdata/reviews.csv' , encoding=\"latin1\")\ngeo_data = gpd.read_file('/kaggle/input/barcelona-airbnb/barcelona_airbnbdata/neighbourhoods.geojson')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:48:23.753733Z","iopub.execute_input":"2025-06-24T07:48:23.754127Z","iopub.status.idle":"2025-06-24T07:48:31.097494Z","shell.execute_reply.started":"2025-06-24T07:48:23.754099Z","shell.execute_reply":"2025-06-24T07:48:31.096797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#visualizing the data\ndef plotData(information = 'price', vmax = 300):\n    #longitudes = listing_data[\"longitude\"]\n    #latitudes = listing_data[\"latitude\"]\n    data = listing_data[information].astype(float)\n\n    mask = data != 0\n    filtered_data = data[mask]\n    filtered_lons = listing_data[\"longitude\"][mask]\n    filtered_lats = listing_data[\"latitude\"][mask]\n    \n    \n    fig, ax = plt.subplots(figsize=(10, 10))\n    geo_data.plot(ax=ax, color='lightgrey', edgecolor='white')\n    \n    sc = ax.scatter(\n        filtered_lons,\n        filtered_lats,\n        c=filtered_data,\n        cmap='RdYlGn_r',\n        s=4,\n        vmin=0,\n        vmax=vmax,\n        alpha=0.4\n    )\n    \n    cbar = plt.colorbar(sc, ax=ax)\n    cbar.set_label(information)\n    plt.xlabel(\"longitude\")\n    plt.ylabel(\"latitude\")\n    \n    ax.set_title(information + \" and location of Airbnbs in Barcelona\")\n    plt.tight_layout()\n    plt.show()\n\nplotData()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:25:13.307637Z","iopub.execute_input":"2025-06-24T07:25:13.307951Z","iopub.status.idle":"2025-06-24T07:25:14.055639Z","shell.execute_reply.started":"2025-06-24T07:25:13.307929Z","shell.execute_reply":"2025-06-24T07:25:14.054610Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Processing the data","metadata":{}},{"cell_type":"code","source":"#listings\n#remove unnecessary data\ncolumns_to_remove = ['listing_url', 'scrape_id', 'host_url', 'host_id', 'host_thumbnail_url' ,  'last_scraped', 'source', 'bathrooms_text', 'calendar_updated', 'calendar_last_scraped']\nlisting_details_data = listing_details_data.drop(columns=columns_to_remove)\nlisting_data = listing_data.drop(columns=['host_id'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:48:34.076734Z","iopub.execute_input":"2025-06-24T07:48:34.077080Z","iopub.status.idle":"2025-06-24T07:48:34.098192Z","shell.execute_reply.started":"2025-06-24T07:48:34.077056Z","shell.execute_reply":"2025-06-24T07:48:34.096938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#find dublicates\ncolumn_names = listing_data.columns.tolist()\nfor column in column_names:\n    \n    if column in listing_details_data.columns.tolist():\n        listing_details_data = listing_details_data.drop(columns = column)\n        \nlisting_data = pd.concat([listing_data, listing_details_data], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:48:35.556120Z","iopub.execute_input":"2025-06-24T07:48:35.556478Z","iopub.status.idle":"2025-06-24T07:48:35.701566Z","shell.execute_reply.started":"2025-06-24T07:48:35.556454Z","shell.execute_reply":"2025-06-24T07:48:35.700715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n        \n\n#reviews\n#remove unnecessary data\ncolumns_to_remove = ['reviewer_id', \"id\", \"reviewer_name\", \"date\"]\nreview_data = review_data.drop(columns = columns_to_remove)\n\nprint(listing_data[:5])\nprint(review_data[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:48:38.418016Z","iopub.execute_input":"2025-06-24T07:48:38.418378Z","iopub.status.idle":"2025-06-24T07:48:38.503100Z","shell.execute_reply.started":"2025-06-24T07:48:38.418352Z","shell.execute_reply":"2025-06-24T07:48:38.502222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handle Nan data","metadata":{}},{"cell_type":"code","source":"#Nan handling\nfor column in listing_data.columns.tolist():\n    isString = False\n    isNumerical = False\n    isCategorial = False\n    for x in listing_data[column]:\n        if type(x) == str:\n            isString = True\n            print(f\"replaced_str: {column}\")\n            break\n        elif type(x) == float or type(x) == int:\n            if x > 1:\n                isNumerical = True\n                print(f\"replaced_numerical: {column}\")\n            break\n        elif type(x) == float or type(x) == int:\n            if x in [0,1]:\n                isCategorial = True\n                print(f\"replaced_categorial: {column}\")\n    \n    if isString:\n        listing_data[column].replace('nan', '')\n        \n    if isNumerical:\n        listing_data[column].replace('nan', 0)\n\n    if isCategorial:\n        listing_data[column].replace('nan', 0)\n        \n\n\nfor column in listing_data.columns:\n    col_type = listing_data[column].dtype\n    \n    if pd.api.types.is_string_dtype(col_type):\n        listing_data[column] = listing_data[column].fillna('')\n    elif pd.api.types.is_numeric_dtype(col_type):\n        unique_vals = listing_data[column].dropna().unique()\n        if set(unique_vals).issubset({0, 1}):\n            listing_data[column] = listing_data[column].fillna(0)\n        else:\n            listing_data[column] = listing_data[column].fillna(0)\n    else:\n        listing_data[column] = listing_data[column].fillna('')\n                \n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:48:40.701862Z","iopub.execute_input":"2025-06-24T07:48:40.702277Z","iopub.status.idle":"2025-06-24T07:48:40.898815Z","shell.execute_reply.started":"2025-06-24T07:48:40.702245Z","shell.execute_reply":"2025-06-24T07:48:40.897760Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handling categorial and image data","metadata":{}},{"cell_type":"code","source":"\n\n\n#expanding listing data amenities list into inidividual rows\ncat_dict = {}\nfor listing in listing_data[\"amenities\"]:\n    listing = listing.replace('[]\"', '').replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', '')\n    listing = listing.split(',')\n    for x in listing:\n        x = x.strip().lower()\n        if x in cat_dict:\n            cat_dict[x] += 1\n        else:\n            cat_dict[x] = 1\n\n\n#take 100 most frequent categories\ntop_amenities = sorted(cat_dict.items(), key=lambda x: x[1], reverse=True)[:100]\nrelevant_categories = [cat for cat, _ in top_amenities]\n    \n\ndef encode_amenities(amenity_str):\n    amenities = amenity_str.replace('[]\"', '').replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', '')\n    amenity_list = [a.strip().lower() for a in amenities.split(',')]\n    row = {}\n    for category in relevant_categories:\n        row[\"amenity_\" + category] = int(any(category in a for a in amenity_list))\n    return row\n    \n\nencoded_df = listing_data[\"amenities\"].apply(encode_amenities).apply(pd.Series)\nencoded_df.columns = [col.replace(\" \", \"_\") for col in encoded_df.columns]\nlisting_data = pd.concat([listing_data, encoded_df], axis=1)\nlisting_data = listing_data.drop(columns='amenities')\n\nprint(listing_data[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:48:44.689586Z","iopub.execute_input":"2025-06-24T07:48:44.689897Z","iopub.status.idle":"2025-06-24T07:48:55.162279Z","shell.execute_reply.started":"2025-06-24T07:48:44.689875Z","shell.execute_reply":"2025-06-24T07:48:55.161249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plotData(\"amenity_free_street_parking\",1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:25:43.218709Z","iopub.execute_input":"2025-06-24T07:25:43.219027Z","iopub.status.idle":"2025-06-24T07:25:43.783575Z","shell.execute_reply.started":"2025-06-24T07:25:43.219000Z","shell.execute_reply":"2025-06-24T07:25:43.782764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Format data in cells","metadata":{}},{"cell_type":"code","source":"#latin1 to closest ascii character\ndef to_ascii(text):\n    if not isinstance(text, str):\n        return text  \n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n\ncolumn_names = listing_data.columns.tolist()\nfor column in column_names:\n    if column not in [\"host_picture_url\", \"picture_url\"]: #Dont mess with links\n        listing_data[column] = listing_data[column].apply(to_ascii)\n\ncolumn_names = review_data.columns.tolist()\nfor column in column_names:\n    review_data[column] = review_data[column].apply(to_ascii)\n\nprint(listing_data.head())\nprint(review_data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:49:00.074048Z","iopub.execute_input":"2025-06-24T07:49:00.074545Z","iopub.status.idle":"2025-06-24T07:49:05.296267Z","shell.execute_reply.started":"2025-06-24T07:49:00.074514Z","shell.execute_reply":"2025-06-24T07:49:05.295224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#license only relevant if true or false\nlisting_data['license'] = listing_data['license'].apply(lambda x: 1 if pd.notna(x) else 0)\n#for column in listing_data.columns.tolist():\n#print(listing_data.columns.tolist())   \nprint(len(listing_data.columns.tolist()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:49:11.754948Z","iopub.execute_input":"2025-06-24T07:49:11.755262Z","iopub.status.idle":"2025-06-24T07:49:11.775207Z","shell.execute_reply.started":"2025-06-24T07:49:11.755239Z","shell.execute_reply":"2025-06-24T07:49:11.774006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#extracting categories\ncategory_dict = {}\ncategory_data_columns = ['neighbourhood_group', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'room_type', 'host_location', 'host_neighbourhood', 'host_response_time', 'property_type']\nfor column in category_data_columns:\n    column_categories = []\n    for x in listing_data[column]:\n        if x not in column_categories:\n            column_categories.append(x)\n    category_dict[column] = column_categories\ncategory_dict\n\n#license only relevant if true or false\nlisting_data['license'] = listing_data['license'].apply(lambda x: 1 if pd.notna(x) else 0)\n#print(listing_data.columns.tolist())   \n\n## t, f to 0, 1\nfor category in listing_data.columns.tolist():\n    if 't' in listing_data[category].tolist() and 'f' in listing_data[category].tolist():\n        print(category)\n        listing_data[category] = listing_data[category].apply(lambda x: 1 if x == 't' else 0)\n        print(listing_data[category].head())\n\n#print(category_dict)\n\n# One-hot encoding for categorical columns\nfor column in category_data_columns:\n    # Get unique categories for this column\n    unique_categories = category_dict[column]\n\n\n    \n    # Create dummy variables for each category\n    for category in unique_categories:\n        cat_name = str(category).replace(' ', '_').replace(',','').lower()\n        listing_data[f'{column}_{cat_name}'] = (listing_data[column] == category).astype(int)\n    \n    listing_data.drop(column, axis=1, inplace=True)\n\n#handle host verifications\nverificat_list = []\nfor listing in listing_data[\"host_verifications\"]:\n    listing = str(listing).replace('[]\"', '').replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', '')\n    listing = listing.split(',')\n    for x in listing:\n        x = x.strip().lower()\n        if x not in verificat_list:\n            verificat_list.append(x)\n\ndef encode_verifications(verifications_str):\n    verifications = str(verifications_str).replace('[]\"', '').replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', '')\n    verifications_list = [v.strip().lower() for v in verifications.split(',')]\n    row = {}\n    categories = []\n    \n    for category in verificat_list:\n        row[\"verificaions_\" + category] = int(any(category in a for a in verifications_list))\n    return row\n    \n\nencoded_df = listing_data[\"host_verifications\"].apply(encode_verifications).apply(pd.Series)\nencoded_df.columns = [col.replace(\" \", \"_\") for col in encoded_df.columns]\nlisting_data = pd.concat([listing_data, encoded_df], axis=1)\nlisting_data = listing_data.drop(columns='host_verifications')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:49:14.086683Z","iopub.execute_input":"2025-06-24T07:49:14.086995Z","iopub.status.idle":"2025-06-24T07:49:20.592093Z","shell.execute_reply.started":"2025-06-24T07:49:14.086974Z","shell.execute_reply":"2025-06-24T07:49:20.591391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(listing_data['property_type_entire_rental_unit'].head())\nplotData(\"neighbourhood_sant_pere_santa_caterina_i_la_ribera\",1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:49:24.908678Z","iopub.execute_input":"2025-06-24T07:49:24.909002Z","iopub.status.idle":"2025-06-24T07:49:25.399646Z","shell.execute_reply.started":"2025-06-24T07:49:24.908978Z","shell.execute_reply":"2025-06-24T07:49:25.398584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(listing_data[\"picture_url\"][100])\nprint(listing_data[\"host_picture_url\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:49:28.866102Z","iopub.execute_input":"2025-06-24T07:49:28.866419Z","iopub.status.idle":"2025-06-24T07:49:28.871911Z","shell.execute_reply.started":"2025-06-24T07:49:28.866396Z","shell.execute_reply":"2025-06-24T07:49:28.870909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#Date to numerical\n#put day, month, year in seperate columns\n#listing_data\ndate_columns = ['last_review', 'host_since', 'first_review']\n#for col in listing_data.columns.tolist():\n    #print(col + \" : \" + str(listing_data[col][0]))\n\n\nlast_review_rows = {\"last_review_day\" : [], \"last_review_month\" : [], \"last_review_year\" : []}\nhost_since_rows = {\"host_since_day\" : [], \"host_since_month\" : [], \"host_since_year\" : []}\nfirst_review_rows = {\"first_review_day\" : [], \"first_review_month\" : [], \"first_review_year\" : []}\n\n\nfor column in date_columns:\n    for x in listing_data[column]:\n        row = [0,0,0]\n        if type(x) != float and x != '':\n            row = x.split('-')\n            row = [int(r) for r in row]\n        #date is in american format !!!\n        if column == \"last_review\":\n            last_review_rows[\"last_review_day\"].append(row[2])\n            last_review_rows[\"last_review_month\"].append(row[1])                \n            last_review_rows[\"last_review_year\"].append(row[0])\n        if column == \"host_since\":\n            host_since_rows[\"host_since_day\"].append(row[2])\n            host_since_rows[\"host_since_month\"].append(row[1])                \n            host_since_rows[\"host_since_year\"].append(row[0])\n        if column == \"host_since\":\n            first_review_rows[\"first_review_day\"].append(row[2])\n            first_review_rows[\"first_review_month\"].append(row[1])                \n            first_review_rows[\"first_review_year\"].append(row[0])\n            \n        \n        \nlast_review_dates = pd.DataFrame.from_dict(last_review_rows)\nhost_since_dates = pd.DataFrame.from_dict(host_since_rows)\nfirst_review_dates = pd.DataFrame.from_dict(first_review_rows)\nprint(last_review_dates.head())\nprint(host_since_dates.head())\nprint(first_review_dates.head())\n\nlisting_data = pd.concat([listing_data, last_review_dates, host_since_dates, first_review_dates], axis = 1)\nlisting_data.drop(columns=date_columns, inplace=True)\nprint(listing_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:49:31.084022Z","iopub.execute_input":"2025-06-24T07:49:31.084660Z","iopub.status.idle":"2025-06-24T07:49:31.480205Z","shell.execute_reply.started":"2025-06-24T07:49:31.084635Z","shell.execute_reply":"2025-06-24T07:49:31.479242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Add Image links\n#f'/kaggle/working/{directory}/{filename}.jpg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:45:01.228494Z","iopub.status.idle":"2025-06-21T14:45:01.228909Z","shell.execute_reply.started":"2025-06-21T14:45:01.228695Z","shell.execute_reply":"2025-06-21T14:45:01.228712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Remove %\n#dont mess with links\ncols_to_clean = listing_data.columns.difference(['picture_url', 'host_picture_url'])\nlisting_data[cols_to_clean] = listing_data[cols_to_clean].replace('%', '', regex=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:49:37.168906Z","iopub.execute_input":"2025-06-24T07:49:37.169225Z","iopub.status.idle":"2025-06-24T07:49:37.610009Z","shell.execute_reply.started":"2025-06-24T07:49:37.169197Z","shell.execute_reply":"2025-06-24T07:49:37.609095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"listing_data[\"host_response_rate\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:26:27.032582Z","iopub.execute_input":"2025-06-24T07:26:27.032893Z","iopub.status.idle":"2025-06-24T07:26:27.041303Z","shell.execute_reply.started":"2025-06-24T07:26:27.032871Z","shell.execute_reply":"2025-06-24T07:26:27.040331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import string\n#listing_data\n#Text to TF-IDF encoding\n#for col in listing_data.columns.tolist():\n#    print(col + \" : \" + str(listing_data[col][0]))\n\ntext_columns = [\"name\", \"description\", \"neighborhood_overview\", \"host_about\", \"host_name\"]\n#for column in text_columns:\n\ndef remove_punctuation(text):\n    if isinstance(text, str):\n        translator = str.maketrans('', '', string.punctuation)\n        return text.translate(translator)\n    else:\n        return text\n\n\n\n#extract word frequencies\ndef getWordFrequencies(column, df = listing_data):\n    word_frequencies = {}\n    for row in df[column]:\n        if type(row) != str:\n            continue\n        words = remove_punctuation(row).split(' ')\n        words = [w.lower() for w in words]\n        for x in words:\n            if x not in word_frequencies:\n                word_frequencies[x] = 1\n            else:\n                word_frequencies[x] += 1\n\n    return word_frequencies\n\n#clean filler words and single letters\ndef cleanToShortWords(word_frequencies, minlen = 3):\n    keysToRemove = []\n    for x in word_frequencies:\n        if len(x) < minlen:\n            keysToRemove.append(x)\n    for x in keysToRemove:\n        word_frequencies.pop(x, None)\n\n\n\n#returns a dataframe that indicates word frequencies\n#input: the number of words to track(int this case 150 most relevant ones)\ndef getWordFreqDataFrame(column, first_word_to_track = 0, size = 150, minlen=3, df = listing_data):\n    freq = getWordFrequencies(column, df)\n    cleanToShortWords(freq, minlen)\n    #extract column names\n    sorted_items = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n    top_categories = [key for key, _ in sorted_items[first_word_to_track:size]]\n\n    word_dict = dict((column + \"_word_freq_\" + cat,df.shape[0] * [0]) for cat in top_categories)\n    \n    for i in range(df.shape[0]):\n    #for row in listing_data[column]:\n        row = df[column][i]\n        if type(row) != str:\n            continue\n        for x in top_categories:\n            if x in row.lower():\n                word_dict[column + \"_word_freq_\" + x][i] += 1\n                if word_dict[column + \"_word_freq_\" + x][i] > 1:\n                    print(\"that actually happens never :)\")\n\n    return pd.DataFrame.from_dict(word_dict)\n    \n\nfor col in text_columns:\n    df = getWordFreqDataFrame(col)\n    listing_data = pd.concat([listing_data, df], axis=1)\n\nlisting_data.drop(columns = text_columns, inplace=True)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:49:40.810009Z","iopub.execute_input":"2025-06-24T07:49:40.810311Z","iopub.status.idle":"2025-06-24T07:49:53.419671Z","shell.execute_reply.started":"2025-06-24T07:49:40.810288Z","shell.execute_reply":"2025-06-24T07:49:53.418630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#reviews \n#Text to TF-IDF encoding\ntext_columns = [\"comments\"]\nfor col in text_columns:\n    df = getWordFreqDataFrame(col, first_word_to_track = 4, size = 300, df = review_data)\n    review_data = pd.concat([review_data, df], axis=1)\nreview_data.drop(columns = text_columns, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:49:59.259042Z","iopub.execute_input":"2025-06-24T07:49:59.259400Z","iopub.status.idle":"2025-06-24T07:54:18.807708Z","shell.execute_reply.started":"2025-06-24T07:49:59.259375Z","shell.execute_reply":"2025-06-24T07:54:18.806620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"review_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:54:40.816233Z","iopub.execute_input":"2025-06-24T07:54:40.816728Z","iopub.status.idle":"2025-06-24T07:54:41.429397Z","shell.execute_reply.started":"2025-06-24T07:54:40.816678Z","shell.execute_reply":"2025-06-24T07:54:41.428349Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Download Images","metadata":{}},{"cell_type":"code","source":"#downloading the images\n#DO NOT RUN AGAIN - THE IMAGES ARE ALREADY DOWNLOADED AND STORED IN /kaggle/working\nos.makedirs('/kaggle/working/picture_url_128x128', exist_ok=True)\nos.makedirs('/kaggle/working/host_picture_url_128x128', exist_ok=True)\n\ndef download_and_resize(url, filename, directory, size=(128, 128)):\n    try:\n        # Download image\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        \n        # Open image\n        img = Image.open(BytesIO(response.content))\n        \n        # Handle alpha channel if exists\n        if img.mode in ('RGBA', 'LA') or (img.mode == 'P' and 'transparency' in img.info):\n            background = Image.new('RGB', img.size, (255, 255, 255))\n            background.paste(img, mask=img.split()[-1])  # Use alpha channel as mask\n            img = background\n        \n        # Convert to RGB if not already (for CMYK, L, etc.)\n        elif img.mode != 'RGB':\n            img = img.convert('RGB')\n            \n        # Resize and save\n        img = img.resize(size, Image.Resampling.LANCZOS)\n        img.save(f'/kaggle/working/{directory}/{filename}.jpg', 'JPEG', quality=85)\n        return True\n        \n    except Exception as e:\n        print(f\"Failed to process {url}: {str(e)}\")\n        return False\n\n# Process all images (adjust batch size as needed)\nfor i, row in listing_data.iterrows():\n    download_and_resize(row['picture_url'], f\"image_{row['id']}\", \"picture_url_128x128\")\n    download_and_resize(row['host_picture_url'], f\"image_{row['id']}\", \"host_picture_url_128x128\")\n\nlisting_data.drop(columns=[\"host_picture_url\", \"picture_url\"], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:37:10.314983Z","iopub.execute_input":"2025-06-21T18:37:10.315295Z","iopub.status.idle":"2025-06-21T20:52:06.298550Z","shell.execute_reply.started":"2025-06-21T18:37:10.315273Z","shell.execute_reply":"2025-06-21T20:52:06.297551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#collaps word frequencies in reviews and merge to listing_data\n#review_data\nreview_data = review_data.groupby('listing_id').sum()\nprint(review_data.shape)\nprint(listing_data.shape)\n\n\nreview_data = review_data.reset_index().rename(columns = {'listing_id': 'id'})\n\nprint(review_data.head())\n\nlisting_data = pd.merge(listing_data, review_data, on='id', how='outer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:54:51.047156Z","iopub.execute_input":"2025-06-24T07:54:51.047521Z","iopub.status.idle":"2025-06-24T07:54:54.117722Z","shell.execute_reply.started":"2025-06-24T07:54:51.047496Z","shell.execute_reply":"2025-06-24T07:54:54.116755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#replace new nans\nfor column in listing_data.columns.tolist():\n    if 'comments_word_freq' in column:\n        listing_data[column].replace(np.nan, 0.0, inplace=True)\n\n#listing_data.drop(columns = [\"id\"], inplace = True)\nlisting_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:56:51.201641Z","iopub.execute_input":"2025-06-24T07:56:51.201929Z","iopub.status.idle":"2025-06-24T07:56:51.368345Z","shell.execute_reply.started":"2025-06-24T07:56:51.201910Z","shell.execute_reply":"2025-06-24T07:56:51.367404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Make numerical \n#listing_data.drop(columns=[\"host_picture_url\", \"picture_url\"], inplace=True)\nlisting_data.replace(\"\", 0, inplace=True)\nlisting_data = listing_data.astype(float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:59:04.703522Z","iopub.execute_input":"2025-06-24T07:59:04.703819Z","iopub.status.idle":"2025-06-24T07:59:04.826680Z","shell.execute_reply.started":"2025-06-24T07:59:04.703802Z","shell.execute_reply":"2025-06-24T07:59:04.825833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Dataframe\nlisting_data.to_csv('/kaggle/working/listing_data.csv', index = False)\n#review_data.to_csv('/kaggle/working/review_data.csv', index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T07:59:38.729302Z","iopub.execute_input":"2025-06-24T07:59:38.729690Z","iopub.status.idle":"2025-06-24T08:00:08.029417Z","shell.execute_reply.started":"2025-06-24T07:59:38.729659Z","shell.execute_reply":"2025-06-24T08:00:08.028165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train the model\n### Linear Regression","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}