{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12265748,"sourceType":"datasetVersion","datasetId":7715214}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\n#progress bar\nfrom tqdm import trange\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.set_default_dtype(torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"use_cuda = True\nuse_cuda = False if not use_cuda else torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\ntorch.cuda.get_device_name(device) if use_cuda else 'cpu'\nprint('Using device', device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading and preparing data","metadata":{}},{"cell_type":"code","source":"# Load and filter data\ndata = pd.read_csv(\"/kaggle/input/airbnbdata-barcelona/listing_data.csv\")\ndata = data.select_dtypes(exclude=['object', 'string'])\ndata = data[[\"id\",\"price\"]]\n\n# Define target column\ntarget_clm = \"price\"\n\n# Drop rows with missing target and missing id\ndata = data.dropna(subset=[target_clm, \"id\"])\n\npicture_dir = \"/kaggle/input/airbnbdata-barcelona/picture_url_128x128/picture_url_128x128\"\nhost_picture_dir = \"/kaggle/input/airbnbdata-barcelona/host_picture_url_128x128/host_picture_url_128x128\"\n\ndef images_exist(img_id):\n    img_id = int(img_id)\n    host_path = os.path.join(host_picture_dir, f\"image_{img_id}.jpg\")\n    picture_path = os.path.join(picture_dir, f\"image_{img_id}.jpg\")\n    return os.path.isfile(host_path) and os.path.isfile(picture_path)\n\n\ndata = data[data['id'].apply(images_exist)].reset_index(drop=True)\n\nprint(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_size = 0.2\ntrain_df, test_df = train_test_split(data, test_size=test_size, random_state=42)\n\n# Normalization stats from training data\n#train_mean = train_df.drop(target_clm, axis=1).mean().astype(np.float32).values\n#train_std = train_df.drop(target_clm, axis=1).std().replace(0, 1).astype(np.float32).values\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class TabularDataset(Dataset):\n    def __init__(self, df, target_clm, mean=None, std=None, normalize=True):\n        self.X = df.drop(target_clm, axis=1).to_numpy(dtype=np.float32)\n        self.y = df[target_clm].to_numpy(dtype=np.float32)\n        self.mean = mean\n        self.std = std\n        self.normalize = normalize\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x = self.X[idx]\n        if self.normalize and self.mean is not None and self.std is not None:\n            x = (x - self.mean) / self.std\n        return torch.tensor(x, dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, dataframe, target_clm, image_dir_host, image_dir_picture, normalize=True, mean = None, std = None, transform=None):\n        self.data = dataframe\n        self.target_clm = target_clm\n        self.image_dir_host = image_dir_host\n        self.image_dir_picture = image_dir_picture\n        self.transform = transform if transform else transforms.ToTensor()\n        self.normalize = normalize\n        self.mean = torch.tensor(mean).view(3, 1, 1)\n        self.std = torch.tensor(std).view(3, 1, 1)\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_id = data['id'].iloc[idx]\n        img_id = int(img_id)\n        target = data[self.target_clm].iloc[idx]\n\n        #Handle missing images\n\n        img_path_host = os.path.join(self.image_dir_host, f\"image_{img_id}.jpg\")\n        img_path_picture = os.path.join(self.image_dir_picture, f\"image_{img_id}.jpg\")\n\n        img_host = Image.open(img_path_host)\n        img_picture = Image.open(img_path_picture)\n\n        img_host = self.transform(img_host)\n        img_picture = self.transform(img_picture)\n\n        if self.normalize and self.mean is not None and self.std is not None:\n            img_host = (img_host - self.mean) / self.std\n            img_picture = (img_picture - self.mean) / self.std        \n        \n        return (img_host, img_picture), torch.tensor(target, dtype=torch.float32)\n\n\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n#typical for normal images\nmean = [0.485, 0.456, 0.406]\nstd  = [0.229, 0.224, 0.225]\n\ntransform = transforms.ToTensor()\n\ntrain_dataset_full = ImageDataset(train_df, target_clm, host_picture_dir, picture_dir, normalize = True, mean =  mean, std = std, transform = transform)\ntest_dataset = ImageDataset(test_df, target_clm, host_picture_dir, picture_dir, normalize = True, mean =  mean, std = std, transform = transform)\n\n\n\ndef show_tensor_image(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    # Clone to avoid modifying original tensor\n    tensor = tensor.clone().detach()\n\n    # Unnormalize\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)\n\n    # Clip to [0, 1] range\n    tensor = tensor.clamp(0, 1)\n\n    # Convert to [H, W, C] for matplotlib\n    np_img = tensor.permute(1, 2, 0).cpu().numpy()\n\n    # Plot\n    plt.imshow(np_img)\n    plt.axis('off')\n    plt.show()\n\n(_,a),_ = train_dataset_full.__getitem__(101)\n\nshow_tensor_image(a)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#split train/val\ntorch.manual_seed(0)\nval_ratio = 0.1\ntrain_dataset, val_dataset = random_split(train_dataset_full, [1 - val_ratio, val_ratio])\n\n#DataLoaders\nbatch_size = 256\ntrain_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nimages, targets = next(iter(train_loader))\n\nprint(images[0].shape)\nprint(images[1].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loosly based on AlexNet\nclass CNNBranch(nn.Module):\n    def __init__(self, in_channels=3):\n        super(CNNBranch, self).__init__()\n\n        self.feature_extractor = nn.Sequential(\n            torch.nn.Conv2d(in_channels = in_channels, out_channels=96, kernel_size=5), #-> 96x124x124\n            torch.nn.ReLU(),\n            torch.nn.AvgPool2d(kernel_size = 2), #-> 96x62x62\n            \n            torch.nn.Conv2d(in_channels = 96, out_channels = 256, kernel_size=3), #-> 256x60x60\n            torch.nn.ReLU(),\n            torch.nn.AvgPool2d(kernel_size = 2), #-> 256x30x30\n            \n            torch.nn.Conv2d(in_channels = 256, out_channels = 384, kernel_size=3), #-> 384x28x28\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3), #-> 384x26x26\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3), #-> 256x24x24\n            torch.nn.ReLU(),\n            torch.nn.AvgPool2d(kernel_size = 2) #-> 256x12x12\n\n        )\n    \n    def forward(self, x):\n        return self.feature_extractor(x)\n\n        \n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PriceCNN(nn.Module):\n    def __init__(self):\n        super(PriceCNN, self).__init__()\n        self.interior_branch = CNNBranch()\n        self.host_branch = CNNBranch()\n\n        self.regressor = nn.Sequential(\n            nn.Flatten(), # -> 2*256*12*12\n            nn.Linear(2 * 256 * 12 * 12, 512), # -> 512\n            nn.ReLU(), \n            nn.Linear(512, 128), # -> 128\n            nn.ReLU(),\n            nn.Linear(128, 1) # -> 1\n        )\n    def forward(self, interior_img, host_img):\n        f1 = self.interior_branch(interior_img)\n        f2 = self.host_img(host_img)\n\n        combined = torch.cat((f1, f2), dim=1) #cat along channel axis\n        return self.regressor(combined)\n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Instantiate model\ninput_dim = train_df.drop(target_clm, axis=1).shape[1]\nmodel = PriceCNN().to(device)\nmodel = model.float()\n# Loss and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Any NaN in features? \", np.isnan(data.drop(target_clm, axis=1).to_numpy()).any())\nprint(\"Any NaN in target? \", np.isnan(data[target_clm].to_numpy()).any())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(dataloader, optimizer, model, loss_fn, device, master_bar):\n    model.train()\n    train_loss = 0.0\n    for batch in dataloader:\n        (interior_img, host_img), price = batch\n        interior_img, host_img, price = interior_img.to(device), host_img.to(device), price.to(device)\n\n        #Forward pass\n        optimizer.zero_grad()\n        outputs = model(interior_img, host_img)\n        loss = loss_fn(outputs, y)\n        train_loss += loss.item()\n        \n        #Backward pass\n        loss.backward()\n        optimizer.step()\n\n        #calculate step quality\n        train_loss += loss.item()\n\n        master_bar.set_description(f\"Epoch {master_bar.n + 1} (Train)\")\n        master_bar.set_postfix(loss=f\"{loss.item():.3f}\")\n\n    return train_loss / len(dataloader)\n\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(dataloader, model, loss_fn, device, master_bar):\n    model.eval()\n    val_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            (interior_img, host_img), price = batch\n            interior_img, host_img, price = interior_img.to(device), host_img.to(device), price.to(device)\n\n            #validate\n            outputs = model(interior_img, host_img)\n            loss = loss_fn(outputs, y)\n            val_loss += loss.item()\n\n            master_bar.set_description(f\"Epoch {master_bar.n + 1} (Validate)\")\n            master_bar.set_postfix(loss=f\"{loss.item():.3f}\")\n    \n    return val_loss / len(dataloader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_training(model, optimizer, loss_function, device, num_epochs, train_dataloader, val_dataloader):\n    train_losses = []\n    val_losses = []\n\n    master_bar = trange(num_epochs, desc=\"Training Epochs\")\n    for epoch in master_bar:\n        train_loss = train(train_dataloader, optimizer, model, loss_function, device, master_bar)\n        val_loss = validate(val_dataloader, model, loss_function, device, master_bar)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n    return train_losses, val_losses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, dataloader, device):\n    model.eval()\n    model.to(device)\n    absolute_errors = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            # Assumes batch is (inputs, targets)\n            (interior_image, host_image), targets = batch\n            interior_image, host_image, targets = interior_image.to(device), host_image.to(device), targets.to(device)\n\n            predictions = model(inputs)\n            abs_errors = abs(predictions - targets)\n            \n            absolute_errors.extend(abs_errors.cpu().numpy())\n\n    return np.array(absolute_errors)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_accuracy(errors, bins = 10, min_range = None, max_range = None):\n\n    if min_range is None:\n        min_range = min(errors)\n    if max_range is None:\n        max_range = max(errors)\n\n    counts, bin_edges = np.histogram(errors, bins=bins, range=(min_range, max_range))\n\n    percentages = 100 * counts / counts.sum()\n\n    bin_widths = bin_edges[1:] - bin_edges[:-1]\n\n    plt.bar(bin_edges[:-1], percentages, width = bin_widths, align = 'edge', edgecolor='black')\n    plt.xlabel(\"Absolute Error\")\n    plt.ylabel(\"Percentage (%)\")\n    plt.title(\"Percentage Distribution of Absolute Errors\")\n    \n    #plt.hist(errors, bins, range=(min_range, max_range), density=True)\n    plt.grid(True)\n    plt.show() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs=10\n\nrun_training(model, optimizer, criterion, device, epochs, train_loader, val_loader)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#evaluate\nerr = evaluate_model(model, test_loader, device)\nplot_accuracy(err, bins = 5, min_range = 0, max_range = 300)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training utility","metadata":{}}]}