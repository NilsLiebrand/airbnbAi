{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12251642,"sourceType":"datasetVersion","datasetId":7719588},{"sourceId":12265748,"sourceType":"datasetVersion","datasetId":7715214}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:42:34.202116Z","iopub.execute_input":"2025-06-24T12:42:34.203008Z","iopub.status.idle":"2025-06-24T12:42:34.206755Z","shell.execute_reply.started":"2025-06-24T12:42:34.202977Z","shell.execute_reply":"2025-06-24T12:42:34.205948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\n#progress bar\nfrom tqdm import trange\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:42:35.524676Z","iopub.execute_input":"2025-06-24T12:42:35.524939Z","iopub.status.idle":"2025-06-24T12:42:35.529368Z","shell.execute_reply.started":"2025-06-24T12:42:35.524921Z","shell.execute_reply":"2025-06-24T12:42:35.528576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.set_default_dtype(torch.float64)\ntorch.cuda.is_available(), torch.backends.cudnn.is_available(), torch.cuda.device_count(), torch.cuda.current_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:42:40.255783Z","iopub.execute_input":"2025-06-24T12:42:40.256556Z","iopub.status.idle":"2025-06-24T12:42:40.262501Z","shell.execute_reply.started":"2025-06-24T12:42:40.256523Z","shell.execute_reply":"2025-06-24T12:42:40.261849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"use_cuda = True\nuse_cuda = False if not use_cuda else torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\ntorch.cuda.get_device_name(device) if use_cuda else 'cpu'\nprint('Using device', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:42:50.405349Z","iopub.execute_input":"2025-06-24T12:42:50.40564Z","iopub.status.idle":"2025-06-24T12:42:50.410562Z","shell.execute_reply.started":"2025-06-24T12:42:50.40562Z","shell.execute_reply":"2025-06-24T12:42:50.409719Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading and preparing data","metadata":{}},{"cell_type":"code","source":"# Load and filter data\ndata = pd.read_csv(\"/kaggle/input/airbnbdata-barcelona/listing_data.csv\")\ndata = data.select_dtypes(exclude=['object', 'string'])\ndata = data.drop(\"id\", axis=1)\n\n# Define target column\ntarget_clm = \"price\"\n\n# Drop rows with missing target\ndata = data.dropna(subset=[target_clm])\n\n# Create an imputer that fills NaNs with the mean of each column\nimputer = SimpleImputer(strategy='mean')\n\n# Fit the imputer on your data and transform it\ndata_imputed = imputer.fit_transform(data)\n\n# Convert back to DataFrame to keep column names\ndata = pd.DataFrame(data_imputed, columns=data.columns)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:42:51.975376Z","iopub.execute_input":"2025-06-24T12:42:51.975651Z","iopub.status.idle":"2025-06-24T12:42:56.377334Z","shell.execute_reply.started":"2025-06-24T12:42:51.975625Z","shell.execute_reply":"2025-06-24T12:42:56.376789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_size = 0.2\ntrain_df, test_df = train_test_split(data, test_size=test_size, random_state=42)\n\n# Normalization stats from training data\ntrain_mean = train_df.drop(target_clm, axis=1).mean().astype(np.float32).values\ntrain_std = train_df.drop(target_clm, axis=1).std().replace(0, 1).astype(np.float32).values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:43:02.546573Z","iopub.execute_input":"2025-06-24T12:43:02.547304Z","iopub.status.idle":"2025-06-24T12:43:03.122583Z","shell.execute_reply.started":"2025-06-24T12:43:02.54728Z","shell.execute_reply":"2025-06-24T12:43:03.121861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class TabularDataset(Dataset):\n    def __init__(self, df, target_clm, mean=None, std=None, normalize=True):\n        self.X = df.drop(target_clm, axis=1).to_numpy(dtype=np.float32)\n        self.y = df[target_clm].to_numpy(dtype=np.float32)\n        self.mean = mean\n        self.std = std\n        self.normalize = normalize\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x = self.X[idx]\n        if self.normalize and self.mean is not None and self.std is not None:\n            x = (x - self.mean) / self.std\n        return torch.tensor(x, dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:43:04.936089Z","iopub.execute_input":"2025-06-24T12:43:04.936372Z","iopub.status.idle":"2025-06-24T12:43:04.942098Z","shell.execute_reply.started":"2025-06-24T12:43:04.936347Z","shell.execute_reply":"2025-06-24T12:43:04.94133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Datasets\ntrain_dataset_full = TabularDataset(train_df, target_clm, train_mean, train_std, normalize=True)\ntest_dataset = TabularDataset(test_df, target_clm, train_mean, train_std, normalize=True)\n\n# Split train/val\ntorch.manual_seed(0)\nval_ratio = 0.1\ntrain_dataset, val_dataset = random_split(train_dataset_full, [1 - val_ratio, val_ratio])\n\n# DataLoaders\nbatch_size = 128\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:43:06.53213Z","iopub.execute_input":"2025-06-24T12:43:06.532714Z","iopub.status.idle":"2025-06-24T12:43:06.687121Z","shell.execute_reply.started":"2025-06-24T12:43:06.532686Z","shell.execute_reply":"2025-06-24T12:43:06.686232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LinearRegressionModel(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, 1)\n\n    def forward(self, x):\n        return self.linear(x).squeeze()  # Squeeze to match target shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:43:08.446708Z","iopub.execute_input":"2025-06-24T12:43:08.446946Z","iopub.status.idle":"2025-06-24T12:43:08.451157Z","shell.execute_reply.started":"2025-06-24T12:43:08.446931Z","shell.execute_reply":"2025-06-24T12:43:08.450405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Instantiate model\ninput_dim = train_df.drop(target_clm, axis=1).shape[1]\nmodel = LinearRegressionModel(input_dim).to(device)\nmodel = model.float()\n# Loss and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:43:10.14673Z","iopub.execute_input":"2025-06-24T12:43:10.147111Z","iopub.status.idle":"2025-06-24T12:43:10.236388Z","shell.execute_reply.started":"2025-06-24T12:43:10.147074Z","shell.execute_reply":"2025-06-24T12:43:10.235843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Any NaN in features? \", np.isnan(data.drop(target_clm, axis=1).to_numpy()).any())\nprint(\"Any NaN in target? \", np.isnan(data[target_clm].to_numpy()).any())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:43:11.792745Z","iopub.execute_input":"2025-06-24T12:43:11.793021Z","iopub.status.idle":"2025-06-24T12:43:11.968833Z","shell.execute_reply.started":"2025-06-24T12:43:11.793001Z","shell.execute_reply":"2025-06-24T12:43:11.968111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(dataloader, optimizer, model, loss_fn, device, master_bar):\n    model.train()\n    train_loss = 0.0\n    for batch in dataloader:\n        X, y = batch\n        X, y = X.to(device), y.to(device)\n\n        #Forward pass\n        optimizer.zero_grad()\n        outputs = model(X)\n        loss = loss_fn(outputs, y)\n\n        #Backward pass\n        loss.backward()\n        optimizer.step()\n\n        #calculate step quality\n        train_loss += loss.item()\n\n        master_bar.set_description(f\"Epoch {master_bar.n + 1} (Train)\")\n        master_bar.set_postfix(loss=f\"{loss.item():.3f}\")\n\n    return train_loss / len(dataloader)\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:43:13.496856Z","iopub.execute_input":"2025-06-24T12:43:13.497138Z","iopub.status.idle":"2025-06-24T12:43:13.502379Z","shell.execute_reply.started":"2025-06-24T12:43:13.497112Z","shell.execute_reply":"2025-06-24T12:43:13.501587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(dataloader, model, loss_fn, device, master_bar):\n    model.eval()\n    val_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            X, y = batch\n            X, y = X.to(device), y.to(device)\n\n            #validate\n            outputs = model(X)\n            loss = loss_fn(outputs, y)\n            val_loss += loss.item()\n\n            master_bar.set_description(f\"Epoch {master_bar.n + 1} (Validate)\")\n            master_bar.set_postfix(loss=f\"{loss.item():.3f}\")\n    \n    return val_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:43:15.162391Z","iopub.execute_input":"2025-06-24T12:43:15.162683Z","iopub.status.idle":"2025-06-24T12:43:15.16818Z","shell.execute_reply.started":"2025-06-24T12:43:15.162664Z","shell.execute_reply":"2025-06-24T12:43:15.167349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_training(model, optimizer, loss_function, device, num_epochs, train_dataloader, val_dataloader):\n    train_losses = []\n    val_losses = []\n\n    master_bar = trange(num_epochs, desc=\"Training Epochs\")\n    for epoch in master_bar:\n        train_loss = train(train_dataloader, optimizer, model, loss_function, device, master_bar)\n        val_loss = validate(val_dataloader, model, loss_function, device, master_bar)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n    return train_losses, val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:43:17.104283Z","iopub.execute_input":"2025-06-24T12:43:17.104552Z","iopub.status.idle":"2025-06-24T12:43:17.109405Z","shell.execute_reply.started":"2025-06-24T12:43:17.104531Z","shell.execute_reply":"2025-06-24T12:43:17.108488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, dataloader, device):\n    model.eval()\n    model.to(device)\n    absolute_errors = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            # Assumes batch is (inputs, targets)\n            inputs, targets = batch\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            predictions = model(inputs)\n            abs_errors = abs(predictions - targets)\n            \n            absolute_errors.extend(abs_errors.cpu().numpy())\n\n    return np.array(absolute_errors)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:43:21.521715Z","iopub.execute_input":"2025-06-24T12:43:21.521991Z","iopub.status.idle":"2025-06-24T12:43:21.52673Z","shell.execute_reply.started":"2025-06-24T12:43:21.521973Z","shell.execute_reply":"2025-06-24T12:43:21.525893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_accuracy(errors, bins = 10, min_range = None, max_range = None):\n\n    if min_range is None:\n        min_range = min(errors)\n    if max_range is None:\n        max_range = max(errors)\n\n    counts, bin_edges = np.histogram(errors, bins=bins, range=(min_range, max_range))\n\n    percentages = 100 * counts / counts.sum()\n\n    bin_widths = bin_edges[1:] - bin_edges[:-1]\n\n    plt.bar(bin_edges[:-1], percentages, width = bin_widths, align = 'edge', edgecolor='black')\n    plt.xlabel(\"Absolute Error\")\n    plt.ylabel(\"Percentage (%)\")\n    plt.title(\"Percentage Distribution of Absolute Errors\")\n    \n    #plt.hist(errors, bins, range=(min_range, max_range), density=True)\n    plt.grid(True)\n    plt.show() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:45:57.572479Z","iopub.execute_input":"2025-06-24T12:45:57.573208Z","iopub.status.idle":"2025-06-24T12:45:57.578504Z","shell.execute_reply.started":"2025-06-24T12:45:57.573175Z","shell.execute_reply":"2025-06-24T12:45:57.577779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs=30\n\nrun_training(model, optimizer, criterion, device, epochs, train_loader, val_loader)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:21:02.491645Z","iopub.execute_input":"2025-06-24T12:21:02.492371Z","iopub.status.idle":"2025-06-24T12:21:35.260882Z","shell.execute_reply.started":"2025-06-24T12:21:02.492346Z","shell.execute_reply":"2025-06-24T12:21:35.26012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#evaluate\nerr = evaluate_model(model, test_loader, device)\nplot_accuracy(err, bins = 5, min_range = 0, max_range = 300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:22:07.123139Z","iopub.execute_input":"2025-06-24T12:22:07.123889Z","iopub.status.idle":"2025-06-24T12:22:07.449437Z","shell.execute_reply.started":"2025-06-24T12:22:07.123863Z","shell.execute_reply":"2025-06-24T12:22:07.448667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Testing other optimizers (Adagrad is very efficient for our sparse data)\noptimizer = torch.optim.Adagrad(model.parameters())\nepochs=30\n\nrun_training(model, optimizer, criterion, device, epochs, train_loader, val_loader)\nerr = evaluate_model(model, test_loader, device)\nplot_accuracy(err, bins = 5, min_range = 0, max_range = 300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:47:52.529147Z","iopub.execute_input":"2025-06-24T12:47:52.529407Z","iopub.status.idle":"2025-06-24T12:47:52.569682Z","shell.execute_reply.started":"2025-06-24T12:47:52.52939Z","shell.execute_reply":"2025-06-24T12:47:52.56866Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training utility","metadata":{}},{"cell_type":"code","source":"epochs = 20\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n\n    for X_batch, y_batch in train_loader:\n        X_batch = X_batch.to(device).float()\n        y_batch = y_batch.to(device).float()\n        \n        preds = model(X_batch)\n        loss = criterion(preds, y_batch)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_val, y_val in val_loader:\n            X_val = X_val.to(device).float()\n            y_val = y_val.to(device).float()\n\n            val_preds = model(X_val)\n            val_loss += criterion(val_preds, y_val).item()\n\n    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:18:26.95088Z","iopub.execute_input":"2025-06-24T12:18:26.951067Z","iopub.status.idle":"2025-06-24T12:18:43.894377Z","shell.execute_reply.started":"2025-06-24T12:18:26.951053Z","shell.execute_reply":"2025-06-24T12:18:43.893666Z"}},"outputs":[],"execution_count":null}]}