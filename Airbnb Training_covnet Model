{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12331747,"sourceType":"datasetVersion","datasetId":7715214}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.191046Z","iopub.execute_input":"2025-07-14T21:00:33.191683Z","iopub.status.idle":"2025-07-14T21:00:33.195218Z","shell.execute_reply.started":"2025-07-14T21:00:33.191659Z","shell.execute_reply":"2025-07-14T21:00:33.194411Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n#progress bar\nfrom tqdm import trange\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.196750Z","iopub.execute_input":"2025-07-14T21:00:33.196944Z","iopub.status.idle":"2025-07-14T21:00:33.215065Z","shell.execute_reply.started":"2025-07-14T21:00:33.196929Z","shell.execute_reply":"2025-07-14T21:00:33.214533Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"torch.set_default_dtype(torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.215732Z","iopub.execute_input":"2025-07-14T21:00:33.215922Z","iopub.status.idle":"2025-07-14T21:00:33.229051Z","shell.execute_reply.started":"2025-07-14T21:00:33.215908Z","shell.execute_reply":"2025-07-14T21:00:33.228409Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"use_cuda = True\nuse_cuda = False if not use_cuda else torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\ntorch.cuda.get_device_name(device) if use_cuda else 'cpu'\nprint('Using device', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.229819Z","iopub.execute_input":"2025-07-14T21:00:33.230386Z","iopub.status.idle":"2025-07-14T21:00:33.244408Z","shell.execute_reply.started":"2025-07-14T21:00:33.230339Z","shell.execute_reply":"2025-07-14T21:00:33.243726Z"}},"outputs":[{"name":"stdout","text":"Using device cuda:0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Loading and preparing data","metadata":{}},{"cell_type":"code","source":"#test_size = 0.2\n#train_df, test_df = train_test_split(data, test_size=test_size, random_state=42)\n\n# Normalization stats from training data\n#train_mean = train_df.drop(target_clm, axis=1).mean().astype(np.float32).values\n#train_std = train_df.drop(target_clm, axis=1).std().replace(0, 1).astype(np.float32).values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:53.167070Z","iopub.execute_input":"2025-07-14T21:00:53.167338Z","iopub.status.idle":"2025-07-14T21:00:53.171524Z","shell.execute_reply.started":"2025-07-14T21:00:53.167320Z","shell.execute_reply":"2025-07-14T21:00:53.170522Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Load and filter data\ndata = pd.read_csv(\"/kaggle/input/airbnbdata-barcelona/listing_data.csv\")\ndata = data.select_dtypes(exclude=['object', 'string'])\n\npicture_dir = \"/kaggle/input/airbnbdata-barcelona/picture_url_128x128/picture_url_128x128\"\nhost_picture_dir = \"/kaggle/input/airbnbdata-barcelona/host_picture_url_128x128/host_picture_url_128x128\"\n\ndef image_exist(img_id):\n    img_id = int(img_id)\n    #host_path = os.path.join(host_picture_dir, f\"image_{img_id}.jpg\")\n    picture_path = os.path.join(picture_dir, f\"image_{img_id}.jpg\")\n    return os.path.isfile(picture_path)\n\ndef host_image_exists(img_id):\n    img_id = int(img_id)\n    host_path = os.path.join(host_picture_dir, f\"image_{img_id}.jpg\")\n    return os.path.isfile(host_path)\n\n\ndata[\"picture_exists\"] = data[\"id\"].apply(image_exist)\ndata[\"host_picture_exists\"] = data[\"id\"].apply(host_image_exists)\n\n# Define target column\ntarget_clm = \"price\"\n\n# Drop rows with missing target and missing id\ndata = data.dropna(subset=[target_clm, \"id\"])\n\n\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n\npca = PCA(n_components=1000)  # You can adjust 300 based on explained variance\ndata_pca = pca.fit_transform(data_scaled)\n\ndata = data.select_dtypes(exclude=['object', 'string'])\n#data = data.drop(\"id\", axis=1)\n\n# Create an imputer that fills NaNs with the mean of each column\ntest_size = 0.2\ntrain_df, test_df = train_test_split(data, test_size=test_size, random_state=42)\n\nimputer = KNNImputer().fit(train_df)\n\n# Fit the imputer on your data and transform it\ntrain_df = pd.DataFrame(imputer.transform(train_df), columns=train_df.columns)\ntest_df = pd.DataFrame(imputer.transform(test_df), columns=test_df.columns)\n\n\n\n\n#data = data[data['id'].apply(images_exist)].reset_index(drop=True)\n\nprint(data)\ndata.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:56.614541Z","iopub.execute_input":"2025-07-14T21:00:56.614815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, dataframe, target_clm, image_dir_host, image_dir_picture, normalize=True, img_mean = None, img_std = None, tab_mean = None, tab_std = None ,transform=None):\n        self.data = dataframe\n        self.target_clm = target_clm\n        self.image_dir_host = image_dir_host\n        self.image_dir_picture = image_dir_picture\n        \n        self.transform = transform if transform else transforms.ToTensor()\n        self.normalize = normalize\n        self.img_mean = torch.tensor(img_mean).view(3, 1, 1) if img_mean is not None else None\n        self.img_std = torch.tensor(img_std).view(3, 1, 1) if img_std is not None else None\n        self.tab_mean = torch.tensor(tab_mean, dtype=torch.float32) if tab_mean is not None else None\n        self.tab_std = torch.tensor(tab_std, dtype=torch.float32) if tab_std is not None else None\n\n        self.tab_features = dataframe.drop(columns = [\"id\", target_clm]).to_numpy(dtype=np.float32)\n        self.targets = dataframe[target_clm].to_numpy\n\n        self.avg_img = Image.new('RGB', (128, 128), (int(0.485 * 255), int(0.456 * 255), int(0.406 * 255)))\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        img_id = int(row['id'])\n        target = data[self.target_clm].iloc[idx]\n\n        \n        img_path_host = os.path.join(self.image_dir_host, f\"image_{img_id}.jpg\")\n        img_path_picture = os.path.join(self.image_dir_picture, f\"image_{img_id}.jpg\")\n\n        img_host = None\n        img_picture = None\n\n        if not os.path.isfile(img_path_host):  \n            #avg_img = Image.new('RGB', (128, 128), (int(0.485 * 255), int(0.456 * 255), int(0.406 * 255)))\n            img_host = self.transform(self.avg_img)\n        else:\n            img_host = Image.open(img_path_host)\n            img_host = self.transform(img_host)\n            \n        if not os.path.isfile(img_path_picture):\n            img_picture = self.transform(self.avg_img)\n\n        else:\n            img_picture = Image.open(img_path_picture)\n            img_picture = self.transform(img_picture)\n\n        \n\n        x_tab = self.tab_features[idx]\n\n        if self.normalize:\n            if self.img_mean is not None and self.img_std is not None:\n                img_host = (img_host - self.img_mean) / self.img_std\n                img_picture = (img_picture - self.img_mean) / self.img_std \n            if self.tab_mean is not None and self.tab_std is not None:\n                x_tab = (x_tab - self.tab_mean.numpy()) / self.tab_std.numpy()\n\n        x_tab = torch.tensor(x_tab, dtype=torch.float32)\n        y = torch.tensor([target], dtype=torch.float32)\n                \n        \n        return (img_host, img_picture, x_tab), y\n\n\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n#typical for normal images\nimg_mean = [0.485, 0.456, 0.406]\nimg_std  = [0.229, 0.224, 0.225]\n\ntab_train_features = train_df.drop(columns=['id', target_clm])\n\ntab_train_mean = tab_train_features.mean().astype(np.float32).values\ntab_train_std = tab_train_features.std().replace(0, 1).astype(np.float32).values\n\ntab_test_features = test_df.drop(columns=['id', target_clm])\n\ntab_test_mean = tab_test_features.mean().astype(np.float32).values\ntab_test_std = tab_test_features.std().replace(0, 1).astype(np.float32).values\n\ntransform = transforms.ToTensor()\n#data argumentation\ntransform = transforms.Compose([\n    #transforms.RandomHorizontalFlip(),\n    #transforms.ColorJitter(brightness=0.2),\n    #transforms.RandomResizedCrop(128),\n    transforms.ToTensor()\n])\n\ntrain_dataset_full = ImageDataset(train_df, target_clm, host_picture_dir, picture_dir, normalize = True, img_mean =  img_mean, img_std = img_std, tab_mean = tab_train_mean, tab_std = tab_train_std, transform = transform)\ntest_dataset = ImageDataset(test_df, target_clm, host_picture_dir, picture_dir, normalize = True, img_mean =  img_mean, img_std = img_std, tab_mean = tab_test_mean, tab_std = tab_test_std ,transform = transform)\n\n\n\ndef show_tensor_image(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    # Clone to avoid modifying original tensor\n    tensor = tensor.clone().detach()\n\n    # Unnormalize\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)\n\n    # Clip to [0, 1] range\n    tensor = tensor.clamp(0, 1)\n\n    # Convert to [H, W, C] for matplotlib\n    np_img = tensor.permute(1, 2, 0).cpu().numpy()\n\n    # Plot\n    plt.imshow(np_img)\n    plt.axis('off')\n    plt.show()\n\n(_,a,b),_ = train_dataset_full.__getitem__(1234)\n\nshow_tensor_image(a)\n\nprint(b.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.332739Z","iopub.execute_input":"2025-07-14T21:00:33.333382Z","iopub.status.idle":"2025-07-14T21:00:33.349769Z","shell.execute_reply.started":"2025-07-14T21:00:33.333337Z","shell.execute_reply":"2025-07-14T21:00:33.348078Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2326243633.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg_std\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtab_train_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_clm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtab_train_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtab_train_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"],"ename":"NameError","evalue":"name 'train_df' is not defined","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"#split train/val\ntorch.manual_seed(0)\nval_ratio = 0.1\n\ntrain_dataset, val_dataset = random_split(train_dataset_full, [1 - val_ratio, val_ratio])\n\n#DataLoaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nimages, targets = next(iter(train_loader))\n\nprint(images[0].shape)\nprint(images[1].shape)\nprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.350118Z","iopub.status.idle":"2025-07-14T21:00:33.350319Z","shell.execute_reply.started":"2025-07-14T21:00:33.350222Z","shell.execute_reply":"2025-07-14T21:00:33.350231Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining the models","metadata":{}},{"cell_type":"code","source":"class ResNetBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResNetBlock, self).__init__()\n        self.conv1 = torch.nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.conv2 = torch.nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n    def forward(self, x):\n        identity = x #(for resnet block adding)\n        out = self.relu(self.conv1(x))\n        out = self.conv2(out)\n        out += identity\n        out = self.relu(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.351848Z","iopub.status.idle":"2025-07-14T21:00:33.352147Z","shell.execute_reply.started":"2025-07-14T21:00:33.351974Z","shell.execute_reply":"2025-07-14T21:00:33.351990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lossly based on resNet 12 https://www.researchgate.net/figure/The-structure-of-ResNet-12_fig1_329954455\nclass CNNBranch(nn.Module):\n    def __init__(self, in_channels=3, num_classes=32):\n        super(CNNBranch, self).__init__()\n\n\n        #added because of strong overfitting\n        self.dropout = nn.Dropout(p=0.1)\n\n        #inital conv and pooling\n        self.stem = nn.Sequential(\n            nn.Conv2d(in_channels = in_channels, out_channels=64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(inplace=True),\n            self.dropout,\n            nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n        )\n\n        #ResNet blocks\n        self.resnet_blocks = nn.Sequential(\n            ResNetBlock(64),\n            ResNetBlock(64),\n            ResNetBlock(64),\n        )\n        \n        # Global average pooling\n        self.global_pool = nn.AdaptiveAvgPool2d((1,1)) #Output: [B, 64, 1, 1]\n\n        # Fully connected Layer\n\n        self.classifier = nn.Linear(64,32)\n    \n\n    \n    def forward(self, x):\n        x = self.stem(x)\n        x = self.resnet_blocks(x)\n        x = self.global_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n        \n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.352985Z","iopub.status.idle":"2025-07-14T21:00:33.353203Z","shell.execute_reply.started":"2025-07-14T21:00:33.353102Z","shell.execute_reply":"2025-07-14T21:00:33.353113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PriceCNN(nn.Module):\n    def __init__(self):\n        super(PriceCNN, self).__init__()\n        self.interior_branch = CNNBranch()\n        self.host_branch = CNNBranch()\n\n        \n    def forward(self, interior_img, host_img):\n        f1 = self.interior_branch(interior_img)\n        f2 = self.host_branch(host_img)\n\n        combined = torch.cat((f1, f2), dim=1) #cat along channel axis\n        return combined.view(combined.size(0), -1) #flatten output\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.354763Z","iopub.status.idle":"2025-07-14T21:00:33.355089Z","shell.execute_reply.started":"2025-07-14T21:00:33.354922Z","shell.execute_reply":"2025-07-14T21:00:33.354938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TabularNN(nn.Module):\n    def __init__(self, input_dim):\n        super(TabularNN, self).__init__()\n        self.dropout = nn.Dropout(p=0.1)\n\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU()\n        )\n        \n    def forward(self, x):\n        return self.mlp(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.355613Z","iopub.status.idle":"2025-07-14T21:00:33.355830Z","shell.execute_reply.started":"2025-07-14T21:00:33.355733Z","shell.execute_reply":"2025-07-14T21:00:33.355742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FullMultimodalModel(nn.Module):\n    def __init__(self, tab_input_dim):\n        super(FullMultimodalModel, self).__init__()\n        self.image_net = PriceCNN()\n        self.tab_net = TabularNN(tab_input_dim)\n\n        img_feature_dim = 2 * 32\n        tab_feature_dim = 64\n\n        self.dropout = nn.Dropout(p=0.1)\n\n        self.regressor = nn.Sequential(\n            nn.Linear(img_feature_dim + tab_feature_dim, 256),\n            nn.ReLU(),\n            self.dropout,\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128,1),\n        )\n    def forward(self, tab_data, interior_img, host_img):\n        img_features = self.image_net(interior_img, host_img)\n        tab_features = self.tab_net(tab_data)\n\n        combined = torch.cat((img_features, tab_features), dim=1)\n        return self.regressor(combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.356917Z","iopub.status.idle":"2025-07-14T21:00:33.357217Z","shell.execute_reply.started":"2025-07-14T21:00:33.357065Z","shell.execute_reply":"2025-07-14T21:00:33.357079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Any NaN in features? \", np.isnan(train_df.drop(target_clm, axis=1).to_numpy()).any())\nprint(\"Any NaN in target? \", np.isnan(train_df[target_clm].to_numpy()).any())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.357908Z","iopub.status.idle":"2025-07-14T21:00:33.358195Z","shell.execute_reply.started":"2025-07-14T21:00:33.358095Z","shell.execute_reply":"2025-07-14T21:00:33.358105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(dataloader, model, loss_fn, device, master_bar):\n    model.eval()\n    val_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            (interior_img, host_img, tab), price = batch\n            interior_img, host_img, tab, price = interior_img.to(device), host_img.to(device), tab.to(device),price.to(device)\n\n            #validate\n            outputs = model(tab, interior_img, host_img)\n            loss = loss_fn(outputs, price)\n            val_loss += loss.item()\n\n            master_bar.set_description(f\"Epoch {master_bar.n + 1} (Validate)\")\n            master_bar.set_postfix(loss=f\"{loss.item():.3f}\")\n    \n    return val_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.359792Z","iopub.status.idle":"2025-07-14T21:00:33.360115Z","shell.execute_reply.started":"2025-07-14T21:00:33.359958Z","shell.execute_reply":"2025-07-14T21:00:33.359972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(dataloader, optimizer, model, loss_fn, device, master_bar):\n    model.train()\n    train_loss = 0.0\n    for batch in dataloader:\n        (interior_img, host_img, tab), price = batch\n        interior_img = interior_img.to(device)\n        host_img = host_img.to(device)\n        tab = tab.to(device)\n        price = price.to(device)\n        #price = price.long()\n\n        #Forward pass\n        optimizer.zero_grad()\n        outputs = model(tab, interior_img, host_img)\n        loss = loss_fn(outputs, price)\n        train_loss += loss.item()\n        \n        #Backward pass\n        loss.backward()\n        optimizer.step()\n\n\n        master_bar.set_description(f\"Epoch {master_bar.n + 1} (Train)\")\n        master_bar.set_postfix(loss=f\"{loss.item():.3f}\")\n\n    return train_loss / len(dataloader)\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.360744Z","iopub.status.idle":"2025-07-14T21:00:33.360992Z","shell.execute_reply.started":"2025-07-14T21:00:33.360891Z","shell.execute_reply":"2025-07-14T21:00:33.360901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_training(model, optimizer, loss_function, device, num_epochs, train_dataloader, val_dataloader):\n    train_losses = []\n    val_losses = []\n\n    master_bar = trange(num_epochs, desc=\"Training Epochs\")\n    for epoch in master_bar:\n        train_loss = train(train_dataloader, optimizer, model, loss_function, device, master_bar)\n        val_loss = validate(val_dataloader, model, loss_function, device, master_bar)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n    return train_losses, val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.362511Z","iopub.status.idle":"2025-07-14T21:00:33.362815Z","shell.execute_reply.started":"2025-07-14T21:00:33.362643Z","shell.execute_reply":"2025-07-14T21:00:33.362658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, dataloader, device):\n    model.eval()\n    model.to(device)\n    absolute_errors = []\n    raw_errors = []\n\n    len(dataloader)\n\n    with torch.no_grad():\n        for batch in dataloader:\n            # Assumes batch is (inputs, targets)\n            (interior_img, host_img, tab), targets = batch\n            interior_img, host_img, tab, targets = interior_img.to(device), host_img.to(device), tab.to(device) ,targets.to(device)\n\n            outputs = model(tab, interior_img, host_img)\n            abs_error = abs(outputs - targets)\n            raw_error = outputs - targets\n            absolute_errors.extend(abs_error.cpu().numpy())\n            raw_errors.extend(raw_error.cpu().numpy())\n\n    return np.array(absolute_errors), np.array(raw_errors)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.363620Z","iopub.status.idle":"2025-07-14T21:00:33.363840Z","shell.execute_reply.started":"2025-07-14T21:00:33.363741Z","shell.execute_reply":"2025-07-14T21:00:33.363750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_accuracy(errors, bins = 10, min_range = None, max_range = None, title=\"\"):\n\n    if min_range is None:\n        min_range = min(errors)\n    if max_range is None:\n        max_range = max(errors)\n\n    counts, bin_edges = np.histogram(errors, bins=bins, range=(min_range, max_range))\n\n    percentages = 100 * counts / counts.sum()\n\n    bin_widths = bin_edges[1:] - bin_edges[:-1]\n\n    plt.bar(bin_edges[:-1], percentages, width = bin_widths, align = 'edge', edgecolor='black')\n    plt.xlabel(\"Absolute Error\")\n    plt.ylabel(\"Percentage (%)\")\n    plt.title(\"Percentage Distribution of \" + title)\n    \n    #plt.hist(errors, bins, range=(min_range, max_range), density=True)\n    plt.grid(True)\n    plt.show() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.364927Z","iopub.status.idle":"2025-07-14T21:00:33.365139Z","shell.execute_reply.started":"2025-07-14T21:00:33.365042Z","shell.execute_reply":"2025-07-14T21:00:33.365052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plotLosses(losses, title = \"\"):\n    plt.plot(range(1, len(losses) + 1),losses, label=title + \" loss\")\n    plt.legend()\n    plt.title(title + \" loss per epoch\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.365852Z","iopub.status.idle":"2025-07-14T21:00:33.366133Z","shell.execute_reply.started":"2025-07-14T21:00:33.365990Z","shell.execute_reply":"2025-07-14T21:00:33.366000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#true vs predicted graph\ndef plot_true_vs_predicted(model, dataloader, device):\n    model.eval()\n    model.to(device)\n    true_y = []\n    predicted_y = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            # Assumes batch is (inputs, targets)\n            (interior_img, host_img, tab), targets = batch\n            interior_img = interior_img.to(device)\n            host_img = host_img.to(device)\n            tab = tab.to(device)\n            targets = targets.to(device)\n\n            outputs = model(tab, interior_img, host_img)\n            \n            # Move tensors to CPU and convert to numpy\n            predicted_y.append(outputs.cpu().numpy())\n            true_y.append(targets.cpu().numpy())\n\n    # Concatenate all batch results\n    true_y = np.concatenate(true_y)\n    predicted_y = np.concatenate(predicted_y)\n\n    plt.figure(figsize=(10, 6))\n    plt.scatter(true_y, predicted_y, alpha=0.5)\n    plt.plot([true_y.min(), true_y.max()], [true_y.min(), true_y.max()], 'k--', lw=2)  # Diagonal line\n    plt.xlabel(\"True Price\")\n    plt.ylabel(\"Predicted Price\")\n    plt.xlim(0, 500)  \n    plt.ylim(0, 500)\n    plt.title(\"True vs Predicted Price\")\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.367183Z","iopub.status.idle":"2025-07-14T21:00:33.367477Z","shell.execute_reply.started":"2025-07-14T21:00:33.367304Z","shell.execute_reply":"2025-07-14T21:00:33.367319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Instantiate model\ninput_dim = train_df.drop(target_clm, axis=1).shape[1]\nmodel = FullMultimodalModel(tab_input_dim=train_df.drop([\"id\", target_clm], axis=1).shape[1])\nmodel.to(device)\n\nmodel = model.float()\n# Loss and optimizer\ncriterion = torch.nn.MSELoss()\n#criterion = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.368020Z","iopub.status.idle":"2025-07-14T21:00:33.368214Z","shell.execute_reply.started":"2025-07-14T21:00:33.368122Z","shell.execute_reply":"2025-07-14T21:00:33.368130Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tweaking the hyperparameters\ntesting different regulisation strengths","metadata":{}},{"cell_type":"code","source":"epochs = 6\ndecay_vals = [0.5,1e-1,1e-2, 1e-3]\n\nall_losses = []\n\n# Train once per decay value and store losses\nfor decay in decay_vals:\n    model = FullMultimodalModel(tab_input_dim=train_df.drop([\"id\", target_clm], axis=1).shape[1])\n    model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=decay)\n    losses = run_training(model, optimizer, criterion, device, epochs, train_loader, val_loader)\n    \n    all_losses.append((decay, losses))  # Store both training and validation losses\n\n# Plot training loss\nplt.figure(figsize=(10, 5))\nfor decay, losses in all_losses:\n    plt.plot(range(1, len(losses[0]) + 1), losses[0], label=f\"Train Loss (decay={decay})\")\nplt.title(\"Training Loss vs Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot validation loss\nplt.figure(figsize=(10, 5))\nfor decay, losses in all_losses:\n    plt.plot(range(1, len(losses[1]) + 1), losses[1], label=f\"Validation Loss (decay={decay})\")\nplt.title(\"Validation Loss vs Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.368829Z","iopub.status.idle":"2025-07-14T21:00:33.369128Z","shell.execute_reply.started":"2025-07-14T21:00:33.368976Z","shell.execute_reply":"2025-07-14T21:00:33.368990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 10\nmodel = FullMultimodalModel(tab_input_dim=train_df.drop([\"id\", target_clm], axis=1).shape[1])\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\ncriterion = nn.HuberLoss(delta=1.0)\nlosses = run_training(model, optimizer, criterion, device, epochs, train_loader, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.370047Z","iopub.status.idle":"2025-07-14T21:00:33.370329Z","shell.execute_reply.started":"2025-07-14T21:00:33.370167Z","shell.execute_reply":"2025-07-14T21:00:33.370179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plotLosses(losses[0], title=\"training\")\nplotLosses(losses[1], title=\"validation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.371617Z","iopub.status.idle":"2025-07-14T21:00:33.371931Z","shell.execute_reply.started":"2025-07-14T21:00:33.371784Z","shell.execute_reply":"2025-07-14T21:00:33.371797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#evaluate\nabs_err, raw_err = evaluate_model(model, test_loader, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.373223Z","iopub.status.idle":"2025-07-14T21:00:33.373505Z","shell.execute_reply.started":"2025-07-14T21:00:33.373383Z","shell.execute_reply":"2025-07-14T21:00:33.373396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_accuracy(abs_err, bins = 10, min_range = 0, max_range = 500, title=\"abs errors\")\nplot_accuracy(raw_err, bins = 20, min_range = -500, max_range = 500, title=\"all errors\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.474059Z","iopub.execute_input":"2025-07-14T21:00:33.474261Z","iopub.status.idle":"2025-07-14T21:00:33.486161Z","shell.execute_reply.started":"2025-07-14T21:00:33.474246Z","shell.execute_reply":"2025-07-14T21:00:33.485199Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3973385557.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"abs errors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all errors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plot_accuracy' is not defined"],"ename":"NameError","evalue":"name 'plot_accuracy' is not defined","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"plot_true_vs_predicted(model, test_loader, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.486775Z","iopub.status.idle":"2025-07-14T21:00:33.486994Z","shell.execute_reply.started":"2025-07-14T21:00:33.486881Z","shell.execute_reply":"2025-07-14T21:00:33.486890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold\nimport pandas as pd\n\n# 1. Create smaller representative sample (20% of data for speed)\nsample_df = train_df.sample(frac=0.8, random_state=42)  # Adjust fraction as needed\n\n# 2. Split into features and target\nX = sample_df.drop(['price'], axis=1)\ny = sample_df['price']\n\n# 3. Reduce feature dimensionality (if >100 features)\n#if len(X.columns) > 100:\n#    print(f\"Original features: {len(X.columns)}\")\n#    selector = VarianceThreshold(threshold=0.01)  # Removes near-constant features\n#    X = pd.DataFrame(selector.fit_transform(X), \n#                    columns=selector.get_feature_names_out())\n#    print(f\"Reduced features: {len(X.columns)}\")\n\n# 4. Create train/validation split (80/20)\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=42\n)\n\n# 5. Train optimized Random Forest\nrf = RandomForestRegressor(\n    n_estimators=50,        # Fewer trees for speed\n    max_depth=10,           # Shallower trees\n    max_features='sqrt',    # Better for high-dimensional data\n    min_samples_leaf=5,     # Prevent overfitting\n    n_jobs=-1,             # Use all CPU cores\n    random_state=42\n).fit(X_train, y_train)\n\n# 6. Evaluate\ntrain_score = rf.score(X_train, y_train)\nval_score = rf.score(X_val, y_val)\nprint(f\"Training R2: {train_score:.3f}\")\nprint(f\"Validation R2: {val_score:.3f}\")\n\n# 7. Feature importance (diagnostics)\nimportances = pd.DataFrame({\n    'feature': X.columns,\n    'importance': rf.feature_importances_\n}).sort_values('importance', ascending=False)\nprint(\"\\nTop 30 features:\")\nprint(importances.head(30))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T21:00:33.488114Z","iopub.status.idle":"2025-07-14T21:00:33.488486Z","shell.execute_reply.started":"2025-07-14T21:00:33.488287Z","shell.execute_reply":"2025-07-14T21:00:33.488306Z"}},"outputs":[],"execution_count":null}]}